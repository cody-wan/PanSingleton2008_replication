{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from scipy.interpolate import CubicSpline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##############   1. prepare raw CDS data (Bloomberg)    #################\n",
    "\n",
    "# Bloomberg\n",
    "\n",
    "COUNTRY_NUM = 15\n",
    "OUTPUT_REPO = \"EXT_data\"\n",
    "\n",
    "for i in ['1', '3', '5']:\n",
    "    CDS1Y = pd.read_excel(\"raw/bbg_cds_data.xlsx\", sheet_name=f\"CDS{i}Y_values\", dtype=str, na_values=['-1'], parse_dates=[0], index_col=0)\n",
    "    # keep non-NA columns\n",
    "    CDS_columns = [str(col) for col in CDS1Y.columns if \"0\" not in str(col)]\n",
    "    CDS1Y = CDS1Y[CDS_columns[:COUNTRY_NUM]].astype(float)\n",
    "    CDS1Y.to_csv(f\"inputs/{OUTPUT_REPO}/CDS{i}Y.csv\")\n",
    "    # CDS1Y.loc['2003-02-01':].iloc[:85].to_csv(f\"inputs/{OUTPUT_REPO}/CDS{i}Y.csv\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "##############   1. prepare raw CDS data (Markit)    #################\n",
    "\n",
    "OUTPUT_REPO = \"MKT_data\"\n",
    "raw = pd.read_csv(\"raw/markit_cds.csv\")\n",
    "tickers = pd.read_csv(\"raw/markit_tickers.txt\", header=None)[0].values\n",
    "\n",
    "\n",
    "for tenor in ['1Y', '3Y', '5Y']:\n",
    "\n",
    "    concat_list = []\n",
    "    # aggregate raw data\n",
    "    # long to wide\n",
    "    # daily to monthly\n",
    "    for j, country in enumerate(tickers):\n",
    "        temp = raw[(raw['Tenor'] == tenor) & (raw['Ticker'] == country)]\n",
    "        temp = temp.set_index(pd.to_datetime(temp['Date'], format=\"%Y%m%d\"))\n",
    "        temp = temp.resample('1M').last()['ParSpread'] * 10000\n",
    "        temp.name = country\n",
    "        # save for concatenation\n",
    "        concat_list.append(temp)\n",
    "\n",
    "    res = pd.concat(concat_list, axis='columns', join='outer')\n",
    "    res.to_csv(f\"inputs/{OUTPUT_REPO}/raw/CDS{tenor}.csv\")\n",
    "    \n",
    "    # clean up\n",
    "    concat_list = []\n",
    "    for j, country in enumerate(tickers):\n",
    "        \n",
    "        # generate rows to have consecutive date values\n",
    "        country_df = res[country].reset_index().dropna()\n",
    "        t0, t1 = country_df['Date'].values[0], country_df['Date'].values[-1]\n",
    "        time_index = pd.date_range(start=t0, end=t1, freq='1M')\n",
    "\n",
    "        # dataframe from t0 to t1 with consecutive dates\n",
    "        country_df = pd.DataFrame(index=time_index)\n",
    "        country_df['Date'] = time_index\n",
    "        country_df[country] = res[country]\n",
    "\n",
    "        # computes pct of missing data in consecutive time period\n",
    "        num_NA = int(country_df[country].isna().sum())\n",
    "        pct_missing = num_NA / len(time_index)\n",
    "\n",
    "        if pct_missing >= 0.05: # >= 5%; take longest consecutive subset of CDS data\n",
    "\n",
    "            # drop na to allow partition of sub df\n",
    "            country_df = country_df.dropna()\n",
    "\n",
    "            # assign True if 1 month apart from subsequent date\n",
    "            # `in_block` gives which data are in consecutive block\n",
    "            in_block = (country_df['Date'].diff()/np.timedelta64(1, 'M')).round() == 1\n",
    "            filtered = country_df.loc[in_block]\n",
    "\n",
    "            # `breaks` indicate the start of each group \n",
    "            # `groups` assign integer to each group\n",
    "            breaks = (filtered['Date'].diff() / np.timedelta64(1, 'M')).round() != 1\n",
    "            groups = breaks.cumsum()\n",
    "\n",
    "            temp = sorted([frame for _, frame in filtered.groupby(groups)], key=lambda x: len(x), reverse=True)[0]\n",
    "            temp = temp.set_index('Date')\n",
    "            # concat_list.append(temp)\n",
    "            # break\n",
    "        else: # do linear interpolate\n",
    "            temp = country_df.interpolate(method='linear', inplace=False)\n",
    "            temp = temp.set_index('Date')\n",
    "\n",
    "        concat_list.append(temp)\n",
    "            \n",
    "\n",
    "    res = pd.concat(concat_list, axis='columns', join='outer')\n",
    "    res.to_csv(f\"inputs/{OUTPUT_REPO}/CDS{tenor}.csv\")\n",
    "\n",
    "\n",
    "    # break\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "##############   2. construct discount value; boostrap Treasury constant maturity curve       #################\n",
    "# partial credit to Steven Zheng\n",
    "\n",
    "PFILE = pd.read_csv(\"raw/FRB_H15.csv\", header=0, skiprows=[1,2,3,4,5], dtype=str, na_values=['ND'], parse_dates=[0])\n",
    "\n",
    "# rename columns using maturity in weeks\n",
    "new_cols = ['date', 4, 13, 26, 52, 104, 156, 260, 364, 520, 1040, 1560]\n",
    "rename_dict = { col: new_cols[i] for i, col in enumerate(PFILE.columns)}\n",
    "PFILE.rename(columns=rename_dict, inplace=True)\n",
    "# set index, and set values to numeric, % to unit less\n",
    "PFILE = PFILE.set_index('date')\n",
    "PFILE = PFILE.astype({col:np.float for col in PFILE.columns}) / 100\n",
    "# compute discount value\n",
    "for col in PFILE.columns:\n",
    "    PFILE[col] = PFILE[col].apply(func= lambda x: 1. / ( (1 + x) ** (float(col) / 52) ))\n",
    "PFILE = PFILE.dropna(axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# interpolate to fill weekly maturities using cubic spline\n",
    "tenor = PFILE.columns.astype(int)\n",
    "inter_PFILE_list = list()\n",
    "\n",
    "for i, row in PFILE.iterrows():\n",
    "    cs = CubicSpline(x=tenor, y=row.values)\n",
    "    inter_PFILE_list.append([float(cs(t)) for t in range(1,261)])\n",
    "\n",
    "inter_PFILE = pd.DataFrame(index=PFILE.index, columns=range(1,261), data=inter_PFILE_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# PFILE\n",
    "inter_PFILE.to_csv(\"inputs/Test_data/PFILE.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ad4badbd540aaf7d72ab39ea74c9da0ec6c3302057d45d66aed1deddbea8dc2d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}